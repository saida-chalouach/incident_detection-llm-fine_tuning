{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/saida-chalouach/incident_detection-llm-fine_tuning/blob/main/Fine_Tuning_Mistral_Incidents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM0mzuuKhZak"
   },
   "source": [
    "# ü§ñ Fine-tuning Mistral pour la D√©tection d'Incidents Web\n",
    "\n",
    "Ce notebook vous guide √† travers toutes les √©tapes du fine-tuning dans Google Colab.\n",
    "\n",
    "## ‚öôÔ∏è Configuration requise\n",
    "- **GPU**: T4 (gratuit) ou A100 (Colab Pro)\n",
    "- **Runtime**: GPU activ√©\n",
    "\n",
    "## üìã √âtapes\n",
    "1. Configuration de l'environnement\n",
    "2. Upload et pr√©paration des donn√©es\n",
    "3. Fine-tuning du mod√®le\n",
    "4. √âvaluation\n",
    "5. Test et utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW_tLZEghZal"
   },
   "source": [
    "## üîß √âTAPE 1: Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM8aGXDOhZal"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "emFg-VB2hZam"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì¶ Installation des d√©pendances...\n",
      "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy==2.1.0 in /usr/local/lib/python3.12/dist-packages (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "‚úÖ Installation termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# Installation des d√©pendances avec versions compatibles pour Colab\n",
    "print(\"üì¶ Installation des d√©pendances...\")\n",
    "\n",
    "# D'abord, installer torch compatible avec l'environnement Colab\n",
    "!pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
    "\n",
    "# Puis installer le reste des packages\n",
    "!pip install -q -U transformers datasets accelerate peft bitsandbytes\n",
    "!pip install pandas==2.2.2 numpy==2.1.0 --break-system-packages\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BtgC_5LrhZam"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Imports r√©ussis\n",
      "üî• PyTorch version: 2.5.1+cu124\n",
      "üéÆ CUDA disponible: True\n",
      "üéØ GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JV1M7VgShZam"
   },
   "source": [
    "## üì§ √âTAPE 2: Upload et pr√©paration des donn√©es\n",
    "\n",
    "**Instructions:**\n",
    "1. Ex√©cutez la cellule ci-dessous\n",
    "2. Cliquez sur \"Choose Files\"\n",
    "3. S√©lectionnez votre fichier `data_stage_2A.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Ty50oFwhZam"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìÇ Upload de votre fichier de donn√©es...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-88b06133-0dc3-49d4-acb5-f9dc2e6a7014\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-88b06133-0dc3-49d4-acb5-f9dc2e6a7014\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving data_stage_2A.txt to data_stage_2A.txt\n",
      "\n",
      "‚úÖ Fichier upload√©: data_stage_2A.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üìÇ Upload de votre fichier de donn√©es...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# R√©cup√©rer le nom du fichier upload√©\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Fichier upload√©: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w1XkLv-WhZan"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìÇ Lecture du fichier: data_stage_2A.txt\n",
      "‚úÖ 1666 enregistrements extraits\n",
      "\n",
      "üìä DataFrame cr√©√©: 1666 lignes, 9 colonnes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  heartbeat_id is_important                    monitor_name monitor_id  \\\n",
       "0     41837805            0  Frontend PiTransfer Standalone         67   \n",
       "1     41837806            0        Frontend PiDF Standalone         69   \n",
       "2     41837807            0                     Socket Chat         35   \n",
       "3     41837808            0              New Media Server 9         59   \n",
       "4     41837809            0                  S3 Minio Store         63   \n",
       "\n",
       "  heartbeat_status heartbeat_msg           heartbeat_time heartbeat_ping  \\\n",
       "0                1      200 - OK  2024-07-05 02:37:28.238             49   \n",
       "1                1      200 - OK  2024-07-05 02:37:28.307             40   \n",
       "2                1      200 - OK  2024-07-05 02:37:28.426             43   \n",
       "3                1      200 - OK  2024-07-05 02:37:30.179             15   \n",
       "4                1      200 - OK  2024-07-05 02:37:30.358             40   \n",
       "\n",
       "  duration  \n",
       "0       60  \n",
       "1       60  \n",
       "2       60  \n",
       "3       60  \n",
       "4       60  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-97e2f447-ec41-42b2-8a73-af7c642abe56\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heartbeat_id</th>\n",
       "      <th>is_important</th>\n",
       "      <th>monitor_name</th>\n",
       "      <th>monitor_id</th>\n",
       "      <th>heartbeat_status</th>\n",
       "      <th>heartbeat_msg</th>\n",
       "      <th>heartbeat_time</th>\n",
       "      <th>heartbeat_ping</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41837805</td>\n",
       "      <td>0</td>\n",
       "      <td>Frontend PiTransfer Standalone</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>200 - OK</td>\n",
       "      <td>2024-07-05 02:37:28.238</td>\n",
       "      <td>49</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41837806</td>\n",
       "      <td>0</td>\n",
       "      <td>Frontend PiDF Standalone</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>200 - OK</td>\n",
       "      <td>2024-07-05 02:37:28.307</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41837807</td>\n",
       "      <td>0</td>\n",
       "      <td>Socket Chat</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>200 - OK</td>\n",
       "      <td>2024-07-05 02:37:28.426</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41837808</td>\n",
       "      <td>0</td>\n",
       "      <td>New Media Server 9</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>200 - OK</td>\n",
       "      <td>2024-07-05 02:37:30.179</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41837809</td>\n",
       "      <td>0</td>\n",
       "      <td>S3 Minio Store</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>200 - OK</td>\n",
       "      <td>2024-07-05 02:37:30.358</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97e2f447-ec41-42b2-8a73-af7c642abe56')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-97e2f447-ec41-42b2-8a73-af7c642abe56 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-97e2f447-ec41-42b2-8a73-af7c642abe56');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1666,\n  \"fields\": [\n    {\n      \"column\": \"heartbeat_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1666,\n        \"samples\": [\n          \"41838979\",\n          \"41839038\",\n          \"41838310\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_important\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"monitor_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"Frontend Modules\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"monitor_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"60\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heartbeat_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heartbeat_msg\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"200 - OK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heartbeat_time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1657,\n        \"samples\": [\n          \"2024-07-05 03:05:51.849\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heartbeat_ping\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 124,\n        \"samples\": [\n          \"95\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"59\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Fonction de parsing des donn√©es\n",
    "def parse_data_file(file_path):\n",
    "    \"\"\"Parse le fichier texte et extrait les donn√©es structur√©es\"\"\"\n",
    "    data = []\n",
    "\n",
    "    print(f\"üìÇ Lecture du fichier: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if line.startswith('A: '):\n",
    "            values = line[3:].split(';')\n",
    "            if len(values) >= 9:\n",
    "                record = {\n",
    "                    'heartbeat_id': values[0],\n",
    "                    'is_important': values[1],\n",
    "                    'monitor_name': values[2],\n",
    "                    'monitor_id': values[3],\n",
    "                    'heartbeat_status': values[4],\n",
    "                    'heartbeat_msg': values[5],\n",
    "                    'heartbeat_time': values[6],\n",
    "                    'heartbeat_ping': values[7],\n",
    "                    'duration': values[8]\n",
    "                }\n",
    "                data.append(record)\n",
    "\n",
    "    print(f\"‚úÖ {len(data)} enregistrements extraits\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Parser les donn√©es\n",
    "df = parse_data_file(data_file)\n",
    "print(f\"\\nüìä DataFrame cr√©√©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EhpoTv7ehZan"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üîç D√©tection des incidents...\n",
      "\n",
      "üìä Statistiques:\n",
      "  Total: 1666 enregistrements\n",
      "  Incidents: 35 (2.1%)\n",
      "  Normaux: 1631 (97.9%)\n"
     ]
    }
   ],
   "source": [
    "# Fonction de d√©tection d'incidents\n",
    "def detect_incident(row):\n",
    "    \"\"\"D√©termine si un heartbeat indique un incident\"\"\"\n",
    "    # Status 0 = Down\n",
    "    if row['heartbeat_status'] == '0':\n",
    "        return True, 'Service down - heartbeat status is 0'\n",
    "\n",
    "    # Message vide ou erreur\n",
    "    msg = row['heartbeat_msg'].strip()\n",
    "    if not msg or msg == '':\n",
    "        return True, 'No response message received'\n",
    "\n",
    "    # Codes d'erreur HTTP\n",
    "    error_codes = ['400', '401', '403', '404', '500', '502', '503', '504']\n",
    "    if any(code in msg for code in error_codes):\n",
    "        return True, f'HTTP error detected: {msg}'\n",
    "\n",
    "    # Ping anormalement √©lev√© (> 250ms)\n",
    "    try:\n",
    "        ping_str = row['heartbeat_ping'].strip()\n",
    "        if ping_str:\n",
    "            ping = float(ping_str)\n",
    "            if ping > 250:\n",
    "                return True, f'High latency detected: {ping}ms'\n",
    "    except (ValueError, AttributeError):\n",
    "        pass\n",
    "\n",
    "    return False, 'Normal operation - all metrics within acceptable range'\n",
    "\n",
    "# Appliquer la d√©tection\n",
    "print(\"üîç D√©tection des incidents...\")\n",
    "df['is_incident'], df['incident_reason'] = zip(*df.apply(detect_incident, axis=1))\n",
    "\n",
    "print(f\"\\nüìä Statistiques:\")\n",
    "print(f\"  Total: {len(df)} enregistrements\")\n",
    "print(f\"  Incidents: {df['is_incident'].sum()} ({df['is_incident'].sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Normaux: {(~df['is_incident']).sum()} ({(~df['is_incident']).sum()/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y6BLh8o3hZan"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üîÑ G√©n√©ration des √©chantillons d'entra√Ænement...\n",
      "‚úÖ 3332 √©chantillons g√©n√©r√©s\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er les √©chantillons d'entra√Ænement\n",
    "def create_training_samples(df, num_variations=2):\n",
    "    \"\"\"Cr√©e des √©chantillons d'entra√Ænement vari√©s\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    print(f\"üîÑ G√©n√©ration des √©chantillons d'entra√Ænement...\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        monitor = row['monitor_name']\n",
    "        status = row['heartbeat_status']\n",
    "        msg = row['heartbeat_msg']\n",
    "        ping = row['heartbeat_ping']\n",
    "        time = row['heartbeat_time']\n",
    "        is_incident = row['is_incident']\n",
    "        reason = row['incident_reason']\n",
    "\n",
    "        # Templates de questions\n",
    "        question_templates = [\n",
    "            f\"Analyser le statut du service {monitor}: status={status}, message={msg}, ping={ping}ms\",\n",
    "            f\"Le service {monitor} fonctionne-t-il correctement? Status: {status}, Response: {msg}\",\n",
    "            f\"V√©rifier l'√©tat de {monitor}. Status={status}, Latency={ping}ms\",\n",
    "            f\"Diagnostic pour {monitor}: {msg}, latence {ping}ms\",\n",
    "        ]\n",
    "\n",
    "        # R√©ponses\n",
    "        if is_incident:\n",
    "            answer_templates = [\n",
    "                f\"üö® INCIDENT D√âTECT√â sur {monitor}. Raison: {reason}. Action requise imm√©diatement.\",\n",
    "                f\"‚ö†Ô∏è Alerte: {monitor} rencontre un probl√®me - {reason}. Intervention n√©cessaire.\",\n",
    "                f\"Probl√®me identifi√© sur {monitor}: {reason}. Statut critique.\",\n",
    "            ]\n",
    "        else:\n",
    "            answer_templates = [\n",
    "                f\"‚úÖ {monitor} fonctionne normalement. Status: {status}, {msg}. Tous les indicateurs sont au vert.\",\n",
    "                f\"Aucun incident d√©tect√©. {monitor} est op√©rationnel. Latence: {ping}ms.\",\n",
    "                f\"Service {monitor} en bon √©tat de fonctionnement. {msg}\",\n",
    "            ]\n",
    "\n",
    "        # G√©n√©rer variations\n",
    "        for _ in range(num_variations):\n",
    "            question = random.choice(question_templates)\n",
    "            answer = random.choice(answer_templates)\n",
    "\n",
    "            samples.append({\n",
    "                'instruction': question,\n",
    "                'output': answer,\n",
    "                'input': '',\n",
    "                'is_incident': is_incident,\n",
    "            })\n",
    "\n",
    "    print(f\"‚úÖ {len(samples)} √©chantillons g√©n√©r√©s\")\n",
    "    return samples\n",
    "\n",
    "# G√©n√©rer les √©chantillons\n",
    "samples = create_training_samples(df, num_variations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C-CmOcJmhZan"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚öñÔ∏è √âquilibrage du dataset...\n",
      "  Incidents: 70\n",
      "  Normaux: 3262\n",
      "\n",
      "üìä Dataset √©quilibr√©: 140 √©chantillons\n",
      "\n",
      "üìà R√©partition:\n",
      "  Train: 112 (80.0%)\n",
      "  Validation: 14 (10.0%)\n",
      "  Test: 14 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# √âquilibrer et diviser le dataset\n",
    "print(\"‚öñÔ∏è √âquilibrage du dataset...\")\n",
    "\n",
    "incident_samples = [s for s in samples if s['is_incident']]\n",
    "normal_samples = [s for s in samples if not s['is_incident']]\n",
    "\n",
    "print(f\"  Incidents: {len(incident_samples)}\")\n",
    "print(f\"  Normaux: {len(normal_samples)}\")\n",
    "\n",
    "# √âquilibrer\n",
    "min_count = min(len(incident_samples), len(normal_samples))\n",
    "incident_samples = random.sample(incident_samples, min_count)\n",
    "normal_samples = random.sample(normal_samples, min_count)\n",
    "\n",
    "all_samples = incident_samples + normal_samples\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "print(f\"\\nüìä Dataset √©quilibr√©: {len(all_samples)} √©chantillons\")\n",
    "\n",
    "# Split train/val/test (80/10/10)\n",
    "train_data, temp_data = train_test_split(\n",
    "    all_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[s['is_incident'] for s in all_samples]\n",
    ")\n",
    "\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=[s['is_incident'] for s in temp_data]\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà R√©partition:\")\n",
    "print(f\"  Train: {len(train_data)} ({len(train_data)/len(all_samples)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_data)} ({len(val_data)/len(all_samples)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_data)} ({len(test_data)/len(all_samples)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RVO-ZBzihZao"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ data/train_data.jsonl sauvegard√©\n",
      "‚úÖ data/val_data.jsonl sauvegard√©\n",
      "‚úÖ data/test_data.jsonl sauvegard√©\n",
      "\n",
      "üéâ Pr√©paration des donn√©es termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les datasets\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "for split_name, split_data in [('train', train_data), ('val', val_data), ('test', test_data)]:\n",
    "    with open(f'data/{split_name}_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for sample in split_data:\n",
    "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "    print(f\"‚úÖ data/{split_name}_data.jsonl sauvegard√©\")\n",
    "\n",
    "print(\"\\nüéâ Pr√©paration des donn√©es termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJBrN4pBhZao"
   },
   "source": [
    "## üèãÔ∏è √âTAPE 3: Fine-tuning du mod√®le\n",
    "\n",
    "Cette √©tape peut prendre **2-4 heures** sur T4 gratuit, ou **30-60 minutes** sur A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eB1z4Aj3hZao"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "üì¶ Chargement de Mistral-7B...\n",
      "============================================================\n",
      "‚è∞ Cela peut prendre 5-10 minutes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration du mod√®le\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "OUTPUT_DIR = \"./mistral-incident-detector\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì¶ Chargement de Mistral-7B...\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚è∞ Cela peut prendre 5-10 minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Installation de bitsandbytes pour la quantification 4-bit\n",
    "print(\"üì¶ Installation de bitsandbytes...\")\n",
    "\n",
    "!pip install -q bitsandbytes>=0.46.1\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Red√©marrez le runtime maintenant\")\n",
    "print(\"Runtime ‚Üí Restart runtime\")"
   ],
   "metadata": {
    "id": "3wqKcSOfpVZi"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì¶ Installation de bitsandbytes...\n",
      "‚úÖ Installation termin√©e!\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Red√©marrez le runtime maintenant\n",
      "Runtime ‚Üí Restart runtime\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Mn6aVzP5hZao"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üî§ Chargement du tokenizer...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0446a373a6124733b6d83e1760b63d5e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee2b865298f04e60b3755623b3eecc06"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a7270ff13ea4d1595017b7ba069a7fb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9d8161b08bf44b39eb4a208a1c45bbd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4d18bdab7814830b27cfa0a2ab5f9e1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üß† Chargement du mod√®le...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32fbbd436c83453c8f18119b4edfe089"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1224ae23deed4067abe282c86033a92c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d9fbfe9b8374815ab5577747e267b50"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c41036e138df47e29498b41f729cedef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3415bf879734574927aa7c5dfa864de"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚úÖ Mod√®le charg√© avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "# Configuration quantification 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "print(\"üî§ Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Charger le mod√®le\n",
    "print(\"üß† Chargement du mod√®le...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le charg√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e1Mqw_SOhZao"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚öôÔ∏è Configuration LoRA...\n",
      "\n",
      "üìä Param√®tres du mod√®le:\n",
      "  Total: 3,794,014,208\n",
      "  Entra√Ænables: 41,943,040 (1.11%)\n"
     ]
    }
   ],
   "source": [
    "# Configuration LoRA\n",
    "print(\"‚öôÔ∏è Configuration LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher les param√®tres\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nüìä Param√®tres du mod√®le:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Entra√Ænables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h2Df0kM2hZao"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìö Chargement des datasets...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c8188f52b0a4a2c86571f47b5ece31e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cffd2babbda41d5a19f6ada9e01f3f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Train: 112 √©chantillons\n",
      "  Validation: 14 √©chantillons\n",
      "\n",
      "üîÑ Tokenization...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/112 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f11737f789d5430cabb65574fefbecc3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ad7086acdf346b395c90639a2bab812"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Tokenization termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# Pr√©parer les datasets\n",
    "print(\"üìö Chargement des datasets...\")\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='data/train_data.jsonl', split='train')\n",
    "val_dataset = load_dataset('json', data_files='data/val_data.jsonl', split='train')\n",
    "\n",
    "print(f\"  Train: {len(train_dataset)} √©chantillons\")\n",
    "print(f\"  Validation: {len(val_dataset)} √©chantillons\")\n",
    "\n",
    "# Fonction de formatage\n",
    "def format_instruction(sample):\n",
    "    return f\"<s>[INST] {sample['instruction']} [/INST] {sample['output']}</s>\"\n",
    "\n",
    "# Fonction de tokenization\n",
    "def tokenize_function(examples):\n",
    "    texts = [format_instruction({\n",
    "        'instruction': inst,\n",
    "        'output': out\n",
    "    }) for inst, out in zip(examples['instruction'], examples['output'])]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokeniser\n",
    "print(\"\\nüîÑ Tokenization...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1LT1geo2hZao"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üèãÔ∏è Configuration de l'entra√Ænement...\n",
      "\n",
      "üìã Param√®tres:\n",
      "  Epochs: 3\n",
      "  Batch size: 4\n",
      "  Learning rate: 0.0002\n",
      "  Effective batch: 16\n"
     ]
    }
   ],
   "source": [
    "# Configuration de l'entra√Ænement\n",
    "print(\"üèãÔ∏è Configuration de l'entra√Ænement...\\n\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "print(f\"üìã Param√®tres:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rpSd3BcghZap"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Trainer cr√©√©!\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er le Trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EIr0HNjFhZap"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "üöÄ D√âBUT DE L'ENTRA√éNEMENT\n",
      "============================================================\n",
      "\n",
      "‚è∞ Dur√©e estim√©e: 2-4 heures sur T4\n",
      "üìä Surveillez les logs ci-dessous\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 09:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ ENTRA√éNEMENT TERMIN√â!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ENTRA√éNEMENT\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚è∞ Dur√©e estim√©e: 2-4 heures sur T4\")\n",
    "print(\"üìä Surveillez les logs ci-dessous\\n\")\n",
    "\n",
    "# Lancer l'entra√Ænement\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ENTRA√éNEMENT TERMIN√â!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "o-bQOLNfhZap"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üíæ Sauvegarde du mod√®le...\n",
      "‚úÖ Mod√®le sauvegard√© dans ./mistral-incident-detector/\n",
      "\n",
      "üìä √âvaluation finale...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:06]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "R√©sultats:\n",
      "  eval_loss: 0.8402\n",
      "  eval_runtime: 8.2076\n",
      "  eval_samples_per_second: 1.7060\n",
      "  eval_steps_per_second: 0.4870\n",
      "  epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le mod√®le\n",
    "print(\"üíæ Sauvegarde du mod√®le...\")\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© dans {OUTPUT_DIR}/\")\n",
    "\n",
    "# √âvaluation finale\n",
    "print(\"\\nüìä √âvaluation finale...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nR√©sultats:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk448k5IhZap"
   },
   "source": [
    "## üìä √âTAPE 4: √âvaluation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MkvOQEXehZap"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Fonctions de pr√©diction pr√™tes\n"
     ]
    }
   ],
   "source": [
    "# Fonction de pr√©diction\n",
    "def predict_incident(instruction, max_new_tokens=150):\n",
    "    \"\"\"Pr√©dit la r√©ponse pour une instruction\"\"\"\n",
    "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"[/INST]\" in full_response:\n",
    "        response = full_response.split(\"[/INST]\")[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "\n",
    "    return response\n",
    "\n",
    "def is_incident_prediction(response):\n",
    "    \"\"\"D√©termine si c'est un incident\"\"\"\n",
    "    response_lower = response.lower()\n",
    "\n",
    "    incident_keywords = [\n",
    "        'incident', 'alerte', 'probl√®me', 'erreur', 'anomalie',\n",
    "        'üö®', '‚ö†Ô∏è', 'down', 'critique', 'panne'\n",
    "    ]\n",
    "\n",
    "    normal_keywords = [\n",
    "        'normal', 'op√©rationnel', 'aucun incident', 'fonctionne',\n",
    "        '‚úÖ', 'ok', 'bon √©tat', 'nominal'\n",
    "    ]\n",
    "\n",
    "    incident_score = sum(1 for kw in incident_keywords if kw in response_lower)\n",
    "    normal_score = sum(1 for kw in normal_keywords if kw in response_lower)\n",
    "\n",
    "    return incident_score > normal_score\n",
    "\n",
    "print(\"‚úÖ Fonctions de pr√©diction pr√™tes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJ7XhDTDhZap"
   },
   "outputs": [],
   "source": [
    "# Test sur quelques exemples\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTS DU MOD√àLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_cases = [\n",
    "    \"Analyser le statut du service MySQL: status=0, message=, ping=ms\",\n",
    "    \"Le service WebSite fonctionne-t-il correctement? Status: 1, Response: 200 - OK\",\n",
    "    \"V√©rifier l'√©tat de Backend API. Status=1, message=500 - Error, Latency=120ms\",\n",
    "    \"Service Sentry: status=1, 200 - OK, ping=350ms\",\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"Test {i}\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    print(f\"\\nüìù Question:\\n   {test}\")\n",
    "\n",
    "    response = predict_incident(test)\n",
    "    is_incident = is_incident_prediction(response)\n",
    "\n",
    "    print(f\"\\nü§ñ R√©ponse:\\n   {response}\")\n",
    "    print(f\"\\nüè∑Ô∏è Classification: {'üö® INCIDENT' if is_incident else '‚úÖ NORMAL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZdyNIwVkhZap"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "üìà √âVALUATION SUR LE TEST SET\n",
      "============================================================\n",
      "\n",
      "üîÑ √âvaluation de 14 √©chantillons...\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚úÖ √âvaluation termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# √âvaluation sur le test set\n",
    "print(\"=\"*60)\n",
    "print(\"üìà √âVALUATION SUR LE TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open('data/test_data.jsonl', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Limiter √† 100 pour la vitesse (retirez [:100] pour tout tester)\n",
    "test_sample = test_data[:100]\n",
    "\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "print(f\"\\nüîÑ √âvaluation de {len(test_sample)} √©chantillons...\\n\")\n",
    "\n",
    "for idx, sample in enumerate(test_sample):\n",
    "    instruction = sample['instruction']\n",
    "    true_label = sample['is_incident']\n",
    "\n",
    "    response = predict_incident(instruction)\n",
    "    pred_label = is_incident_prediction(response)\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "    ground_truth.append(true_label)\n",
    "\n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"  Progression: {idx + 1}/{len(test_sample)}\")\n",
    "\n",
    "print(\"\\n‚úÖ √âvaluation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TdxeN4aThZap"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "üìä R√âSULTATS FINAUX\n",
      "============================================================\n",
      "\n",
      "üìà M√©triques:\n",
      "  Accuracy:  0.929 (92.9%)\n",
      "  Precision: 1.000\n",
      "  Recall:    0.857\n",
      "  F1-Score:  0.923\n",
      "\n",
      "üéØ Matrice de confusion:\n",
      "                 Pr√©dit Normal  Pr√©dit Incident\n",
      "  Vrai Normal            7             0\n",
      "  Vrai Incident          1             6\n",
      "\n",
      "üìù Interpr√©tation:\n",
      "  ‚úÖ Vrais positifs: 6 - Incidents correctement d√©tect√©s\n",
      "  ‚úÖ Vrais n√©gatifs: 7 - Normaux correctement identifi√©s\n",
      "  ‚ùå Faux positifs: 0 - Fausses alertes\n",
      "  ‚ùå Faux n√©gatifs: 1 - Incidents manqu√©s\n",
      "\n",
      "üéâ EXCELLENT! Le mod√®le est pr√™t pour la production!\n"
     ]
    }
   ],
   "source": [
    "# Calculer les m√©triques\n",
    "accuracy = accuracy_score(ground_truth, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    ground_truth, predictions, average='binary', zero_division=0\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(ground_truth, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä R√âSULTATS FINAUX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìà M√©triques:\")\n",
    "print(f\"  Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1-Score:  {f1:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Matrice de confusion:\")\n",
    "print(f\"                 Pr√©dit Normal  Pr√©dit Incident\")\n",
    "print(f\"  Vrai Normal         {tn:4d}          {fp:4d}\")\n",
    "print(f\"  Vrai Incident       {fn:4d}          {tp:4d}\")\n",
    "\n",
    "print(f\"\\nüìù Interpr√©tation:\")\n",
    "print(f\"  ‚úÖ Vrais positifs: {tp} - Incidents correctement d√©tect√©s\")\n",
    "print(f\"  ‚úÖ Vrais n√©gatifs: {tn} - Normaux correctement identifi√©s\")\n",
    "print(f\"  ‚ùå Faux positifs: {fp} - Fausses alertes\")\n",
    "print(f\"  ‚ùå Faux n√©gatifs: {fn} - Incidents manqu√©s\")\n",
    "\n",
    "if accuracy >= 0.90:\n",
    "    print(\"\\nüéâ EXCELLENT! Le mod√®le est pr√™t pour la production!\")\n",
    "elif accuracy >= 0.80:\n",
    "    print(\"\\n‚úÖ BON! Performances acceptables, quelques am√©liorations possibles.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Performances insuffisantes. Entra√Ænez plus longtemps ou ajoutez des donn√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqndK4HhhZaq"
   },
   "source": [
    "## üíæ √âTAPE 5: T√©l√©charger le mod√®le\n",
    "\n",
    "T√©l√©chargez votre mod√®le fine-tun√© pour l'utiliser plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_0WiCveEhZaq"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì¶ Compression du mod√®le...\n",
      "  adding: mistral-incident-detector/ (stored 0%)\n",
      "  adding: mistral-incident-detector/adapter_config.json (deflated 58%)\n",
      "  adding: mistral-incident-detector/README.md (deflated 66%)\n",
      "  adding: mistral-incident-detector/chat_template.jinja (deflated 64%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/ (stored 0%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/adapter_config.json (deflated 58%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/README.md (deflated 66%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/chat_template.jinja (deflated 64%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/trainer_state.json (deflated 56%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/scheduler.pt (deflated 56%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/tokenizer_config.json (deflated 48%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/adapter_model.safetensors (deflated 8%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/training_args.bin (deflated 51%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/rng_state.pth (deflated 25%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/scaler.pt (deflated 60%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/optimizer.pt (deflated 10%)\n",
      "  adding: mistral-incident-detector/checkpoint-21/tokenizer.json (deflated 85%)\n",
      "  adding: mistral-incident-detector/tokenizer_config.json (deflated 48%)\n",
      "  adding: mistral-incident-detector/adapter_model.safetensors (deflated 8%)\n",
      "  adding: mistral-incident-detector/training_args.bin (deflated 51%)\n",
      "  adding: mistral-incident-detector/tokenizer.json (deflated 85%)\n",
      "‚úÖ Compression termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# Compresser le mod√®le\n",
    "print(\"üì¶ Compression du mod√®le...\")\n",
    "!zip -r mistral-incident-detector.zip mistral-incident-detector/\n",
    "print(\"‚úÖ Compression termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BxUIOiekhZaq"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚¨áÔ∏è T√©l√©chargement du mod√®le...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_86898149-35ed-4e7e-a3f4-a14a0076feb9\", \"mistral-incident-detector.zip\", 386692257)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚úÖ T√©l√©chargement lanc√©! V√©rifiez vos t√©l√©chargements.\n"
     ]
    }
   ],
   "source": [
    "# T√©l√©charger\n",
    "from google.colab import files\n",
    "\n",
    "print(\"‚¨áÔ∏è T√©l√©chargement du mod√®le...\")\n",
    "files.download('mistral-incident-detector.zip')\n",
    "print(\"\\n‚úÖ T√©l√©chargement lanc√©! V√©rifiez vos t√©l√©chargements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQJbj_g4hZaq"
   },
   "source": [
    "## üéâ F√âLICITATIONS!\n",
    "\n",
    "Vous avez termin√© le fine-tuning de Mistral pour la d√©tection d'incidents!\n",
    "\n",
    "### Prochaines √©tapes:\n",
    "1. ‚úÖ T√©l√©chargez votre mod√®le (voir cellule ci-dessus)\n",
    "2. ‚úÖ Testez avec vos propres donn√©es\n",
    "3. ‚úÖ D√©ployez en production (API Flask, FastAPI, etc.)\n",
    "\n",
    "### Pour sauvegarder sur Google Drive:\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp -r mistral-incident-detector /content/drive/MyDrive/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}